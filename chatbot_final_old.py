# -*- coding: utf-8 -*-
"""chatbot_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Or4WQujpZMm6EssoPFbZLITNB4kc1BYg
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet --upgrade langchain langchain-community langchainhub langchain-openai
!pip install langchain-unstructured
!pip install unstructured-client
!pip install -qU chromadb langchain-chroma
!pip install transformers sentence-transformers langchain
#!pip install unstructured==0.5.6
!pip install pytesseract
!pip install backoff
!pip install chromadb
!pip install langgraph
!pip install python-pptx
!pip install --upgrade unstructured[pptx]
!pip install Cmake
!pip install poppler-utils
!pip install --upgrade setuptools wheel
!pip install tesseract-ocr
!pip install libreoffice
!pip install pytesseract Pillow
!pip install langchain_core
!pip install bitsandbytes
!pip install -U langchain-huggingface



!pip install cffi --only-binary :all:
!pip install python-libmagic
!pip install python-poppler
!pip install --upgrade nltk
#!pip install unstructured[local-inference]

# Warning control
import warnings
import os
warnings.filterwarnings('ignore')
##


##
import chromadb
from flask import Flask
import time
import random
##

# Pre-process the pdf file
os.environ['USER_AGENT'] = 'TERNAbot/1.0'
from langchain_core.documents import Document
from unstructured.documents.elements import Image
from uuid import uuid4
from PIL import Image as PILImage
from io import BytesIO
from langdetect import detect
#from langchain.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings

from langchain.prompts.prompt import PromptTemplate
from langchain_openai import OpenAI, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompt_values import ChatPromptValue
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import RetrievalQA


##
from unstructured_client import UnstructuredClient
from unstructured_client.models import shared
from unstructured_client.utils import BackoffStrategy, RetryConfig
from unstructured_client.models.errors import SDKError
from unstructured.documents.elements import Title, Text, NarrativeText, Table, ListItem, Image


from unstructured.chunking.title import chunk_by_title
#from unstructured.partition.md import partition_md
#from unstructured.partition.pptx import partition_pptx
#from unstructured.staging.base import dict_to_elements
import numpy as np
import openai
import backoff
import pytesseract
from sklearn.metrics.pairwise import cosine_similarity
#from langchain.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma

from langchain_community.document_loaders import UnstructuredPowerPointLoader  # Unstructured PowerPoint loader
from unstructured.staging.base import dict_to_elements # Assuming this is needed for Yolox output
from typing import Sequence

import bs4
import chromadb
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
#from langchain_community.document_loaders import WebBaseLoader
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
#from langchain_core.vectorstores import InMemoryVectorStore
#from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph, MessagesState
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
#from sentence_transformers import SentenceTransformer
from langchain.llms import HuggingFacePipeline
from transformers import pipeline, T5Tokenizer, MT5ForQuestionAnswering
from langchain.embeddings import HuggingFaceEmbeddings
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, BitsAndBytesConfig
import nltk
import streamlit as st
# Set the NLTK data directory to a custom path
nltk.data.path.append('/root/nltk_data/')

# Then download the Punkt tokenizer
nltk.download('punkt', download_dir='/root/nltk_data/')

model_name = "BAAI/bge-m3"
#"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# Initialize embeddings globally
embeddings = HuggingFaceEmbeddings(model_name=model_name)

os.environ["LANGCHAIN_TRACING_V2"]="true"
os.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"]="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"
os.environ["LANGCHAIN_PROJECT"]="TERNA-chatbot"

# Set your OpenAI API key
#os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["OPENAI_API_KEY"] = "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"
DLAI_API_KEY = "kBZuTqG5e1jMTj2zt4QAKkz77rAMmi"
DLAI_API_URL = "https://api.unstructuredapp.io/general/v0/general"
s = UnstructuredClient(
    api_key_auth=DLAI_API_KEY,
    server_url=DLAI_API_URL,
        retry_config=RetryConfig(
        strategy="backoff",
        retry_connection_errors=True,
        backoff=BackoffStrategy(
            initial_interval=500,
            max_interval=60000,
            exponent=1.5,
            max_elapsed_time=900000,
        )
    )
)

persist_directory="./chroma_langchain_db"
'''llm = SentenceTransformer(model_name)

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=1000,
    openai_api_key=os.environ["OPENAI_API_KEY"],
    timeout=None,
    max_retries=5,
    # api_key="...",  # if you prefer to pass api key in directly instaed of using env vars
    # base_url="...",
    # organization="...",
    # other params...
)'''
chat_history = []

# Set-up Unstructured API credentials


llm_model_name = "meta-llama/Llama-3.1-8B-Instruct"
#llm_model_name = "EleutherAI/gpt-neo-125M"


# Gated model: Login with a HF token with gated access permission
!huggingface-cli login

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)
model = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)





'''llm_model_name = "google/mt5-small"

tokenizer = T5Tokenizer.from_pretrained(llm_model_name)
model = T5ForConditionalGeneration.from_pretrained(llm_model_name)

llm_model_name = "facebook/mbart-large-cc25"

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-cc25")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-cc25")'''

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.2,
    do_sample=True,
    repetition_penalty=1.1,
    return_full_text=False,
    max_new_tokens=200,
    eos_token_id=terminators,
)

# Step 5: Wrap the pipeline with HuggingFacePipeline for easier integration with other tasks
llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
client = OpenAI()

# Backoff for handling rate limiting in completions
'''@backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=5)
def completions_with_backoff(**kwargs):
    """Handle rate-limited completions from OpenAI."""
    return openai.Completion.create(**kwargs)'''

# Backoff for embedding generation
@backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=10)
def generate_embedding(chunks):
    """Generate embedding for the user query with rate limit handling."""
    uuids = [str(uuid4()) for _ in range(len(chunks))]


    # Save the vector store
    vector_store = Chroma(
    collection_name="chroma_index",
    embedding_function=embeddings,
    persist_directory=persist_directory,  # Where to save data locally, remove if not necessary
    )
    vector_store.add_documents(documents=chunks, ids=uuids)

    return vector_store

# Backoff for embedding generation
@backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=5)
def generate_query_embedding(query):
    """Generate embedding for the user query with rate limit handling."""
    query_embedding = embeddings.embed_query(query)  # Generate the embedding
    return query_embedding

# Function to perform OCR on images
def perform_ocr_on_image(image_data):
    # If image_data is a file path
    if isinstance(image_data, str):
        image = PILImage.open(image_data)
    # If image_data is binary data, convert it to an image
    elif isinstance(image_data, bytes):
        image = PILImage.open(BytesIO(image_data))
    else:
        raise ValueError("Unsupported image data format.")

    # Perform OCR
    text = pytesseract.image_to_string(image)
    return text

# Function to partition PPTX files using Yolox model and extract elements
def partition_pptx_with_yolox(filename):
    """Partition PPTX file using Yolox for high-resolution image processing."""
    with open(filename, "rb") as f:
        files = shared.Files(
            content=f.read(),
            file_name=filename,
        )

    req = shared.PartitionParameters(
        files=files,
        strategy=shared.Strategy.HI_RES,  # High-resolution strategy
        hi_res_model_name="yolox",  # Yolox model
        languages =  ['eng', 'ita'], # an error might occur here
    )

    try:
        resp = s.general.partition(req)

        img_elements = dict_to_elements(resp.elements)  # Extract elements
        return img_elements
    except SDKError as e:
        st.error(e)
        return []

# Define a helper function to handle complex metadata conversion
def filter_or_convert_metadata(metadata):
    # Iterate through metadata dictionary and process values
    for key, value in metadata.items():
        if isinstance(value, list):
            # Convert lists to comma-separated strings
            metadata[key] = ', '.join(map(str, value))
        elif isinstance(value, dict):
            # Filter out dictionaries or complex types using helper method
            metadata[key] = filter_complex_metadata(metadata[key])
    return metadata

# Function to load and split the PPTX file
def load_and_split_document_with_images(filename):
    """Load a PPTX document and extract images and text."""

    # Use UnstructuredPowerPointLoader for structured data
    loader = UnstructuredPowerPointLoader(filename, mode="elements")
    pptx_elements = loader.load()  # Load the PPTX elements
    unstructured_image_elements = partition_pptx_with_yolox(filename)
    '''chunked_pptx_data = chunk_by_title(pptx_elements,
                                  # maximum for chunk size
                                  max_characters=512,
                                  # You can choose to combine consecutive elements that are too small
                                  # e.g. individual list items
                                  combine_text_under_n_chars=200,
                                  )
    chunked_unstructured_image_elements = chunk_by_title(unstructured_image_elements,
                                  # maximum for chunk size
                                  max_characters=512,
                                  # You can choose to combine consecutive elements that are too small
                                  # e.g. individual list items
                                  combine_text_under_n_chars=200,
                                  )'''
    for chunk in pptx_elements:
                    # Filter or convert the metadata
        #chunk.metadata = chunk.metadata.to_dict()
        chunk.metadata = filter_or_convert_metadata(chunk.metadata)
        #chunk.metadata.source = chunk.metadata.filename
        del chunk.metadata["languages"]

    for chunk in unstructured_image_elements:
                    # Filter or convert the metadata
        chunk.metadata = chunk.metadata.to_dict()
        chunk.metadata = filter_or_convert_metadata(chunk.metadata)
        #chunk.metadata.source = chunk.metadata.filename
        #del chunk.metadata["languages"]
    documents = process_pptx_data(pptx_elements)
    docs = filter_complex_metadata(documents)
    final_documents = process_unstructured_image_data(unstructured_image_elements, documents)


    return final_documents  # Return the documents list

# Function to find similar chunks based on cosine similarity
def find_similar_chunks(chunks, query_embedding, k=5):
    """Calculate cosine similarity and retrieve the top k similar chunks."""
    chunk_embeddings = embeddings.embed_documents([chunk.page_content for chunk in chunks])
    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]
    similar_indices = np.argsort(similarities)[-k:][::-1]  # Get the indices of the top k similarities
        # Retrieve the actual similar chunks using the indices
    similar_chunks = [chunks[idx] for idx in similar_indices]
    documents = process_pptx_data(similar_chunks)

    return documents

# Function to generate a response from the language model
def generate_response(prompt, max_tokens=100):
    """Generate a response from the language model using backoff for rate limiting."""
    response = completions_with_backoff(
        model="gpt-4",  # Specify your model
        prompt=prompt,
        max_tokens=max_tokens  # Set max token limit here
    )
    return response.choices[0].text.strip()  # Return the generated text

# Define the retry decorator
def retry_with_exponential_backoff(
    func,
    initial_delay: float = 1,
    exponential_base: float = 2,
    jitter: bool = True,
    max_retries: int = 0,
    timeout: int = 120,
    errors: tuple = (openai.RateLimitError,),
):
    """Retry a function with exponential backoff."""
    def wrapper(*args, **kwargs):
        num_retries = 0
        delay = initial_delay
        start_time = time.time()  # Record start time
        while True:
            try:
                result = func(*args, **kwargs)
                #print(f"Result: {result}")
                if result:
                    return result
            except errors as e:
                st.error(f"Error: {e}")
                num_retries += 1
                if num_retries > max_retries:
                    raise Exception(f"Maximum number of retries ({max_retries}) exceeded.")
                    return None  # Return None if max retries exceeded

                # Check if timeout is exceeded
                elapsed_time = time.time() - start_time
                if elapsed_time > timeout:
                    st.error(f"Operation timed out after {timeout} seconds. Returning None.")
                    return None  # Return None if timeout exceeded
                delay *= exponential_base * (1 + jitter * random.random())
                time.sleep(delay)
            except Exception as e:
                raise e
        return result
    return wrapper

# Apply retry to completions or vector store calls
@retry_with_exponential_backoff
def completions_with_backoff(**kwargs):
    return client.chat.completions.create(**kwargs)

# Main QA pipeline
def qa_pipeline(vectorstore, query):
    """Conduct a question-answering session using the provided vectorstore and manage chat history within the same function."""

    # Initialize an empty chat history
    chat_history = []

    # Step 1: Conversation loop
    while True:
        # Determine the question to ask
        if query:
            question = query
        else:
            question = st.text_input("Ask a question (type 'exit' to quit): ")  # For console-based interaction

        # Check for exit command
        if question.lower() == 'exit':
            st.write("Ending conversation.")
            break
        if not question:
            st.write("Please ask a valid question.")
            continue
        with st.spinner("Retrieving relevant documents..."):
            # Step 2: Set up the retriever for the vector store
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 1}  # Retrieve the top 1 most similar document
            )

            # Step 3: Retrieve context from the vector store
            results = retriever.invoke(question)

        if not results:
            st.write("No relevant information found.")
            continue

        format_docs = " ".join([doc.page_content for doc in results])


        # Step 4: Define the system prompt with the retrieved context
        # Step 4: Define the system prompt with the retrieved context
        template = """
            Use the following pieces of context to answer the question at the end.
            If you don't know the answer, just say that you do not have the relevant information needed to provide a verified answer, don't try to make up an answer.
            When providing an answer, aim for clarity and precision. Position yourself as a knowledgeable authority on the topic, but also be mindful to explain the information in a manner that is accessible and comprehensible to those without a technical background.
            Always say "Do you have any more questions pertaining to this instrument?" at the end of the answer. Mention the source of the retrieved context.
            If questions are asked in Italian, only then answer in Italian; otherwise, always answer in English.
            {context}
            Question: {input}
            Helpful Answer:"""

        '''prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )'''
        prompt = ChatPromptTemplate.from_template(template)

        #custom_rag_prompt = PromptTemplate.from_template(template)
        chat_history.append(f"Human: {question}")
        # If this is not the first question, append the previous AI answer as well


        # Combine chat history and context
        history_and_context = "\n".join(chat_history) + "\n" + format_docs
        with st.spinner("Generating response..."):
            # Prepare the messages including chat history and context
            messages = prompt.format_messages(
                input=question,        # The user's question
                context=history_and_context  # Combine chat history and retrieved context
            )
            rag_chain = (
                {"context": retriever, "input": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
            )
            # Step 5: Set up the prompt for QA with Langchain's ChatPromptTemplate

            # Step 6: Create the question-answering chain
            #question_answer_chain = create_stuff_documents_chain(llm, prompt)
            # Step 7: Create the retrieval chain
            #rag_chain = create_retrieval_chain(retriever, question_answer_chain)

            # Step 8: Generate the response
            #response = rag_chain.invoke(question)
            @retry_with_exponential_backoff
            def get_response(input_data):
                return rag_chain.invoke(input_data)

            # Get the response for the current user question
            print(format_docs)
            response = get_response(question)

            # Step 9: Update the chat history with the current exchange
            '''chat_history.extend(
                [
                    HumanMessage(content=question),    # Store user's question
                    AIMessage(content=response["answer"]),  # Store assistant's response
                ]
            )'''
            if chat_history:
                chat_history.append(f"AI: {response}")
            # Step 10: Check if the response uses external information
            # Forcefully ensure that the response is only from the retrieved context
            '''if "source" in response["answer"].lower() or "I don't know" in response["answer"]:
                # If the response mentions external sources, rephrase the response
                print("Assistant: I'm only able to answer based on the information retrieved from the context. I don't have the answer to that.")
            else:
                # Display the assistant's response'''
            st.write(f"Assistant: {response}")

        # Step 11: Option to continue or end the conversation
        exit_prompt = st.text_input("Do you want to ask another question? (yes/no): ").strip().lower()
        if exit_prompt == 'no':
            st.write("Ending conversation.")
            break
        elif exit_prompt == 'yes':
            query = None  # Reset query for the next question
            continue



# Function to initialize or load vector store
def load_or_initialize_vector_store(embeddings, search_query, search_url):
    try:
        # Attempt to load an existing vector store
        vector_store = Chroma(collection_name='chroma_index', persist_directory=persist_directory, embedding_function=embeddings)  # Using Chroma, replace with FAISS if necessary

        if vector_store:
            return vector_store

        else:
            st.write("No vector store found, initializing a new one.")
            chunks = load_and_split_document_with_images(search_url)

            # Initialize a new vector store
            # Save the new vector store

            vector_store = generate_embedding(chunks)
            return vector_store   # Return the new vector store

    except Exception as e:
        st.error(f"Error loading vector store: {e}")
        # If there's an error, create a new vector store from the provided chunks
        chunks = load_and_split_document_with_images(search_url)

        vector_store = generate_embedding(chunks)
        return vector_store  # Return the new vector store

def process_pptx_data(pptx_elements):
    # Create Document instances

    '''documents = []
    for element in pptx_elements:
        doc = Document(
            page_content=element.page_content,
            metadata=element.metadata,
            id=str(uuid4())  # Generate a unique ID for each document
        )
        documents.append(doc)'''
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    documents = []
    for element in pptx_elements:
        chunks = text_splitter.split_text(element.page_content)  # Split the content
        for chunk in chunks:
            if hasattr(element.metadata, 'to_dict'):
                  element.metadata = element.metadata.to_dict()  # Convert ElementMetadata to a dictionary
            else:
                  pass
            element.metadata["source"] = element.metadata["filename"]
            #del element.metadata["languages"]
            doc = Document(
                page_content=chunk,
                metadata=element.metadata,  # Retain the original metadata
                id=str(uuid4())
            )

            documents.append(doc)

    return documents

def process_unstructured_image_data(yolox_elements, documents):
    # Iterate through the elements and extract values based on their type
    for element in yolox_elements:
        if isinstance(element, Image):

          if hasattr(element, 'filepath'):
              #print(f"Image file path: {element.filepath}")
              image_data = perform_ocr_on_image(element.filepath)
              # Add the extracted image data to the document
              doc = Document(
                  page_content=image_data,
                  metadata=element.metadata,
                  id=str(uuid4())  # Generate a unique ID for each document
              )
              documents.append(doc)

          # Perform OCR on the image file if the file path exists

          elif element.__dict__['text']:
              # Add the extracted image data to the document
              if hasattr(element.metadata, 'to_dict'):
                  metadata_dict = element.metadata.to_dict()  # Convert ElementMetadata to a dictionary
              else:
                  metadata_dict = element.metadata
              doc = Document(
                  page_content=element.text,
                  metadata=metadata_dict,
                  id=str(uuid4())  # Generate a unique ID for each document
              )
              documents.append(doc)
          else:
              st.write("No valid information found from the image.")


    # Return the list of extracted values
    return documents



def ensure_english(llm, prompt):
    response = llm(prompt)
    while detect(response) != 'en':
        response = llm(prompt)
    return response


# Main function to tie everything together
def main(file_path, query=None, max_tokens=1000):
    #while True:
    st.markdown(
        """
        <style>
        .main {
            background-color: #f5f5f5; /* Light grey background */
            padding: 20px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    if st.button("Ask Question") and user_query:


          # Exit the chat if user types 'exit'
          '''if user_input.lower() == 'exit':
              print("Goodbye!")
              break'''
    if not query:
          # Get user input
          query = st.text_input("You: ")
    if st.button("Ask Question") and query:
          # If the button is clicked and the query is not empty, process the query
            with st.spinner('Processing your query...'):

                # Step 1: Check if vectore exists and if exists then if embedded chunk already exists in the vectorstore. Then Load and split the document, including handling images and OCR
                vector_store = load_or_initialize_vector_store(embeddings, query, search_url)
                if vector_store:
                      # Perform similarity search with the query
                      results = vector_store.similarity_search_with_score(query, k=1)
                      #NEED TO WORK HERE
                      if not results:
                          # If results are found, process and return them
                          '''for res, score in results:
                              print(f"* [SIM={score:3f}] {res.page_content} [{res.metadata}]")

                          context = " ".join([snippet.page_content for snippet in results])
                          response = completions_with_backoff(prompt=f"{search_query}\nContext: {context}", max_tokens=100)'''
                          print("No results found for the search query. Adding new information to database.")
                          # Update the vector store if no results were found
                          chunks = load_and_split_document_with_images(file_path) # etar ekta bebostha korte hobe
                          # Initialize a new vector store
                          # Save the new vector store

                          query_embedding = generate_query_embedding(query)
                          new_chunks = find_similar_chunks(chunks, query_embedding, k=5)
                          uuids = [str(uuid4()) for _ in range(len(new_chunks))]

                          vector_store.add_documents(documents=new_chunks, ids=uuids)

                      else:
                          pass
                  # Save the updated vector store
                else:
                      st.error("Error: Vector Store not found! Creating and loading...")
                      # Update the vector store if no results were found
                      chunks = load_and_split_document_with_images(file_path)

                      # Initialize a new vector store
                      # Save the new vector store
                      vector_store = generate_embedding(chunks)


                  # Step 2: Generate embedding for the query with rate limit handling
                  #query_embedding = generate_query_embedding(query)

                  # Step 3: Find the most similar chunks based on the query embedding
                  #similar_indices, similarities = find_similar_chunks(chunks, query_embedding)

                  # Step 4: Store the processed chunks in the vector store (Chroma)
                  #vector_store = store_in_vector_store(chunks)

                  # Step 5: Perform the question-answering process using the vector store and query


                '''answer = qa_pipeline(vector_store, query)
                  if answer:
                    print(query)
                    print("Generated Answer:")
                    print(answer['answer'])
                    print(answer['context'])
                    query = None
                  continue'''
                qa_pipeline(vector_store, query)




# Define the Streamlit app
def main_streamlit():
    # The Streamlit UI is initialized automatically when the app runs
    # Add a custom background or logo with markdown
    st.markdown(
        """
        <style>
        .main {
            background-color: #f5f5f5; /* Light grey background */
            padding: 20px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # You can add a logo or header image as well
    #st.image("path/to/your/logo.png", width=150)
    # Title of the app
    st.title("AI-Powered TERNA Chatbot")

    # A brief description of what the app does
    st.write("Welcome! You can start chatting with me (type 'exit' to quit). Ask a question about the pre-loaded PPTX file.")

    # Step 1: Create a text input field for the user's query
    user_query = st.text_input("Enter your question:")

    # Step 2: Create a button that the user can click to submit the query
    if st.button("Ask Question") and user_query:
        # If the button is clicked and the query is not empty, process the query
        with st.spinner('Processing your query...'):
            # Run your main chatbot function here with the hardcoded PPTX file path and user query
            pptx_file_path = 'path/to/your/hardcoded/pptx/file.pptx'  # Your hardcoded PPTX file path
            answer = main(pptx_file_path, user_query)

            # Step 3: Display the chatbot's answer
            if answer:
                st.write(f"**Answer**: {answer}")
            else:
                st.write("Sorry, I couldn't find a relevant answer.")

# Run the Streamlit app
if __name__ == "__main__":
    st.title("AI-Powered TERNA Chatbot")

    # A brief description of what the app does
    st.write("Welcome! You can start chatting with me (type 'exit' to quit). Ask a question about the pre-loaded PPTX files.")
    #st.write("Welcome! You can start chatting with me (type 'exit' to quit).")
    search_query = "Are aphids a pest?"
    search_url = "/main.pptx"  # Path to your document file
    main(search_url)